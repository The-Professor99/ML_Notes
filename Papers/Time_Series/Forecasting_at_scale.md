Forecasting at Scale: Paper by Sean J. Taylor and Benjamin Letham @Facebook - Fbprophet paper. Published 2017)

#### Themes observed in the practice of creating business forecasts:
- Completely automatic forecasting techniques can be hard to tune and are often too inflexible to incorporate useful assumptions or heuristics.
- The **analysts responsible for data science tasks throughout an organization typically have deep domain expertise about the specific products or services that they support, but often do not have training in time series forecasting.** Analysts who can produce high quality forecasts are thus quite rare because forecasting is a specialized skill requiring substantial experience.

#### Approach
The researchers used the analyst-in-the-loop method to approach business forecasting at scale. They start by modeling the time series using a flexible specification that has a straightforward human interpretation for each of the parameters,then produce forecasts for this model and a set of reasonable baselines across a variety of historical simulated forecast dates, and evaluate forecast performance. **When there is poor performance or other aspects of the forecasts warrant human intervention, they flag these potential problems to a human analyst in a prioritized order.** The analyst can then inspect the forecast and potentially adjust the model based on this feedback.

Common features of many business time series include: multiple strong seasonalities, trend changes, outliers and holiday effects. Seasonal effects naturally arise and can be expected in time series generated by human actions and the time series being impacted by new products or market changes result in a change in trend, for instance, a 5-day work week can produce effects on a time series that repeat each week, while vacation schedules and school breaks can produce effects that repeat each year. When these effects are not adequately modeled, the forecast will be poor hence we would want to be able to tune the parameters of the model used to better model the data. Tuning these parameters require a thorough understanding of how the underlying model work that a typical analyst may not have.

The model introduced in the prophet: Prophet, is designed to handle the common features of business time series and it also has intuitive parameters that can be adjusted without knowing the details of the underlying model.

**Prophet equation**
$$ y_{(t)}  = g_{(t)} + s_{(t)} + h_{(t)} + \epsilon_t$$

Where 
- $g_{(t)}$ is the trend function which models non-periodic changes in the value of the time series
- $s_{(t)}$ represents the periodic changes(e.g weekly and yearly seasonality) and
- $h_{(t)}$ represents the effects of holidays which occur on potentially irregular schedules over one or more days.
- The error term $\epsilon_t$ represents any idiosyncratic changes which are not accommodated by the model

#### The trend model
The researchers implemented two trend models that cover many facebook applications: a saturating growth model, and a piecewise linear model
1. Nonlinear Saturating Growth: For growth forecasting, the core component of the data generating process is a model for how the population has grown and how it is expected to continue growing. **Population growth in natural ecosystems have a nonlinear growth that saturates at a carrying capacity and as an example, the carrying capacity for the number of facebook users in a particular area might be the number of people who have access to the internet.** This sort of growth is typically modeled using the logistic growth model, which in its most basic form is $$g_{(t)}  = {C \over  1 + exp(-k(t-m))}$$
With 
- C as the carrying capacity
- k as the growth rate
- m as an offset parameter

**At facebook, 2 important aspects of growth are not captured. First, the carrying capacity is not constant - as the number of people in the world who have access to the internet increases, so does the growth ceiling. They thus replaced the fixed capacity C with a time varying capacity C(t), analysts often have insight into market sizes and can set these accordingly and there may also be external data sources that can provide carrying capacities. Second, the growth rate is not constant. New products can profoundly alter the rate of growth in a region, so the model should be able to incorporate a varying rate inorder to fit historical data.**

#### Seasonality
Fourier series was used to provide a flexible model of periodic effects. Where P is the regular period a time series is expected to have(eg P = 365.25 for yearly data or P = 7 for weekly data), arbitrary smooth seasonal effects was approximated with a standard fourier series as given below. The intercept term can be left out because they also simultaneously fit a trend component.
$$s_{(t)}  = \sum_{n=1}^N({a_ncos ({2\pi nt \over P}}) + b_nsin ({2\pi nt \over P}))$$

#### Holidays and Events
**Holidays and events provide large, somewhat predictable shocks to many business time series and often do not follow a periodic pattern, so their effects are not well modeled by a smooth cycle.** For instance, thanksgiving in the US occur on the fourth thursday in November, The super bowl occurs on a Sunday in January or February that is difficult to declare programmatically. the impact of a particular holiday on the time series is often similar year after year so it is important to incorporate it into the forecast. Prophet is designed to allow the analyst to provide a custom list of past and future events, identified by the event or holiday's unique name. The researchers included a column for country inorder to keep a country specific list of holidays in addition to global holidays. For a given forecasting problem, a union of the global set of holidays and country specific ones is used. Incorporating this list of holidays into the model is made straightforward by assuming that the effects of holidays are independent. **It is often important to include effects for a window of days around a particular holiday,** eg the weekend of Thanksgiving. To account for that, the researchers included additional parameters for the days surrounding the holiday, essentially treating each of the days in the window around the holiday as a holiday itself.

#### Model fitting
An important benefit of the decomposable model employed in this research is that it allows the ability the look at each component of the forecast separately. This provides a useful tool for analysts to gain insight into their forecasting problem besides just producing a prediction.
Two parameters _tau_ and _sigma_ serve as controls for the amount of regularization on the model changepoints and seasonality respectively. Regularization is important for both of these to avoid overfitting, however, there likely is not enough historical data to select the best regularization parameters via cross validation. For this reason, default values that are appropriate for most forecasting problems were set and when these parameters need to be optimized, it happens with the analyst in the loop.

#### Analyst-in-the-loop modeling
In the prophet model specification, there are several places where analysts can alter the model to apply their expertise and external knowledge without requiring any understanding of the underlying statistics. These include:
- Capacities: Analysts may have external data for the total market size and can apply that knowledge directly by specifying capacities
- Changepoints: Known dates of changepoints, such as dates of product changes
- Holidays and seasonality: Relevant holiday dates and the applicable time scales of seasonality.
- Smoothing parameters: The seasonality and holiday smoothing parameters allow the analyst to tell the model how much of the historical seasonal variation is expected in the future.

With good visualization tools, analysts can use these parameters to improve the model fit. When the model fit is plotted over historical data, it is quickly apparent if there were changepoints that were missed by the automatic changepoint selection. Visualization provides many other opportunities for fruitful human intervention: linear trend or logistic growth, identifying time scales of seasonality and identifying outlying time periods that should be removed from fitting, etc. All of these interventions can be made without statistical expertise and are important ways for analysts to apply their insights or domain knowledge.

The forecasting literature often makes the distinction between statistical forecasts, which are based on models fit to historical data, and judgemental forecasts(also called managerial forecasts), which human experts produce using whatever process they have learned tends to work well for a specific time series. Each of these approaches has their advantages. Statistical forecasts require less domain knowledge and effort from human forecasters, and they can scale to many forecasts very easily. Judgemental forecasts include more information and can be more responsive to changing conditions but can require intensive work by analysts. The analyst-in-the-loop modeling approach used in this research attempts to blend the advantages of statistical and judgemental forecasts by focusing analyst effort on improving the model when necessary rather than directly producing forecasts through some unstated procedure. Typical scaling of forecasting would rely on fully automated procedures, but **judgemental forecasts have been shown to be highly accurate in many applications**-(Read more on this, check papers_to_read: Judgmental forecasting: a review of progress over the last 25 years by Lawrence etal). As such, the proposed model lets analysts apply judgement to forecasts through a small set of intuitive model parameters and options, while retaining the ability to fall back on fully automated statistical forecasting when necessary.

#### Use of Baseline forecasts
Simplistic forecasts that make strong assumption about the underlying process but that can produce a reasonable forecast in practice was prefered. The researchers found it useful to compare simplistic models(last value and sample mean) as well as the automated forecasting procedures such as auto-arima.

#### Modeling forecast accuracy
Mean absolute percentage error(MAPE) was preferred in this research for its interpretability.

#### Simulated Historical forecasts (see side notes for more info)
**It is difficult to use a method like cross validation to perform model selection and evaluation because the observations are not exchangeable - we cannot simply randomly partition the data**, as such, simulated historical forecasts(SHFs) were used by producing K forecasts at various cutoff points in the history, choosen such that the horizons lie with the history and the total error can be evaluated. SHFs simulate the errors that should have been made had the forecasting method been used at those points in the past. This method has the advantage of being simple, easy to explain to analysts and decision makers and relatively uncontroversial for generating insight into forecast errors. There are two main issues to be aware of when using the SHF methodology to evaluate and compare forecasting approaches:
- The more simulated forecasts is made, the more correlated their estimates of error are. Eg, in the extreme case of a simulated forecast for each day in the history, the forecasts are unlikely to have changed much given an additional day of information and the errors from one day to the next would be nearly identical. While correlated estimates do not introduce bias into the estimation of model accuracy, they do produce less useful information and slow down forecast evaluation. On the other hand, If we make very few simulated forecasts, we then have fewer observations of historical forecasting errors on which to base model selection. **As a heuristic, for a forecast horizon H, a simulated forecast was made for every H/2 periods**

###### Ways SHF can be used to identify likely problems with forecasts
- When the forecast has large errors relative to the baselines, the model may be misspecified, Analysts can adjust the trend model or the seasonality, as needed.
- Large errors for all methods on a particular date are suggestive of outliers. Analysts can identify outliers and remove them
- When the SHF error for a method increases sharply from one cutoff to the next, it could indicate that the data generating process has changed. Adding changepoints or modeling different phases separately may address the issue.





### Side notes:
##### Simulated forecasts
Simulated forecasts are a technique used in forecasting to estimate how a forecasting model would have performed in the past ahd how it might perform in the future based on historical data. This is mostly used as it is difficult to use a method like cross validation to perform model selection and evaluation because the observations are not exchangeable - we cannot simply randomly partition the data. Simulated forecasts work in the following way:
1. A model is trained using training data.
2. The model is evaluated on a test dataset.
3. Simulate forecasts: At this point, you essentially pretend that you're at some point in the past, and you start making forecasts for each time step from this period as though you were forecasting in real-time, comparing the simulated forecast to the actual observed value for that time step. This allows for the calculation of the error or the difference between the forecasted value and the actual value and the evaluation of how well the model performs. 

Given that the point in the past where simulated forecasts may begin sometimes include points with the training dataset, it would seem like it could lead to overfitting since the training dataset is data seen by the model during training, the critical safeguard against overfitting is the evaluation(testing) phase. If the model performs well on the testing data, it suggests that it can make accurate predictions on unseen data, including when simulating forecasts on historical data.