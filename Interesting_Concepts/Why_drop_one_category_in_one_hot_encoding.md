Regression models like LinearRegression work best when, when using one-hot-encoding, one category level is dropped from the encoded categorical variables.  This is often referred to as "dummy variable encoding" or "dummy variable trap" and is mostly done to prevent multicollinearity and make the model interpretable. When you one-hot encode a categorical variable with n levels, including all n levels as separate predictors would lead to perfect multicollinearity because only one of them can be "one"(1) at a time and the others would be "off". When one category level is dropped, not only does it addresses this multicollinearity, it also simplifies the interpretation of the model coefficients. The dropped category serves as the reference or baseline, and the remaining categories represent deviations from this baseline. This makes it easier to understand the effect of each category on the target variable. The choice of which category to drop is somewhat arbitrary and doesn't impact the model's predictive performance. Any category can be chosen as the reference category but it is good practice to select the model that makes the most sense in the context of your analysis. For example, in the case of the days of the week, you might choose sunday as the reference day because it's often considered the start or end of the week in many cultures.

One important distinction between dropping off one category level when one-hot encoding(eg dropping off Sunday in days of the week) and dropping off one or more predictor variables to address multicollinearity(eg, if number of bedrooms highly correlates with number of bathrooms and one is to be dropped) is this, in one hot encoding, the decision to remove a category level is less about removing a category due to multicollinearity and more about establishing a baseline or  reference point for interpreting the model's coefficients. It doesnt impact the model's predictive performance but rather, simplifies the interpretation of the coefficients.
