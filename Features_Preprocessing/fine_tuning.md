### Hyperparameter Tuning
Hyper-parameters are parameters that are not directly learnt within estimators. In sklearn, they are passed as arguments to the constructor of the estimator classes and it is possible and recommended to search the hyperparameter space for the best cross validation score. Data preparation steps can also be treated as hyperparameters, eg, a grid search can be setup to automatically find out whether or not to add a feature you are not sure about. It may similarly also be used to automatically find the best way to handle outliers, missing features, feature selection, etc.

#### Methods of tuning the hyperparameters of an estimator
1. Exhaustive Grid Search
The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. For instance, the following param_grid:

param_grid = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
 ]
 
specifies that two grids should be explored: one with a linear kernel and C values in [1, 10, 100, 1000], and the second one with an RBF kernel, and the cross-product of C values ranging in [1, 10, 100, 1000] and gamma values in [0.001, 0.0001].

2. Randomized Search
While using the grid search approach is fine when exploring relatively few combinations([it is currently the most widely used method for parameter optimization](https://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-search), other search methods such as RandomizedSearchCV have more favourable properties and may be preferable to use instead(especially when the hyperparameter search space is large). This class can be used in much the same way as GridSeachCV but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration(Each setting is sampled from a distribution over possible parameter values). One of the benefits of this approach is that simply by setting the number of iterations(n_iter), one has more control over the computing budget to be allocated to the hyperparameter search. Specifying how parameters would be sampled is done using a dictionary, very similar to specifying parameters for GridSearchCV, for each parameter, either a distribution over possible values or a list of discrete choices(which will be sampled uniformly) can be specified. If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.

#### Searching for optimal parameters with successive halving
Sklearn also provides HalvingGridSearchCV and HalvingRandomSearchCV estimators that can be used to search a parameter space using successive halving. Successive halving (SH) is like a tournament among candidate parameter combinations. It is an iterative selection process where all candidates (the parameter combinations) are evaluated with a small amount of resources at the first iteration. Only some of these candidates are selected for the next iteration, which will be allocated more resources. For parameter tuning, the resource is typically the number of training samples, but it can also be an arbitrary numeric parameter such as n_estimators in a random forest. I understand this best as a "survival of the fittest" kinda thing as only a subset of candidates ‘survive’ until the last iteration. These are the candidates that have consistently ranked among the top-scoring candidates across all iterations. Each iteration is allocated an increasing amount of resources per candidate, here the number of samples. [See image here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_successive_halving_iterations.html)

In sklearn.model_selection.Halving**,the factor (> 1) parameter controls the rate at which the resources grow, and the rate at which the number of candidates decreases. In each iteration, the number of resources per candidate is multiplied by factor and the number of candidates is divided by the same factor. Along with resource and min_resources, factor is the most important parameter to control the search, though **a value of 3 usually works well.** factor effectively controls the number of iterations in HalvingGridSearchCV and the number of candidates (by default) and iterations in HalvingRandomSearchCV. These estimators are still experimental and their predictions and their API might change without any deprecation cycle. **To use them, you need to explicitly import enable_halving_search_cv**
##### Choosing min_resources and the number of candidates
Beside `factor`, the two main parameters that influence the behaviour of a successive halving search are the `min_resources` parameter, and the number of candidates (or parameter combinations) that are evaluated. `min_resources` is the amount of resources allocated at the first iteration for each candidate.

Consider a case where the resource is the number of samples, and where we have 1000 samples. In theory, with min_resources=10 and factor=2, we are able to run at most 7 iterations with the following number of samples: [10, 20, 40, 80, 160, 320, 640].
But depending on the number of candidates, we might run less than 7 iterations: if we start with a small number of candidates, the last iteration might use less than 640 samples, which means not using all the available resources (samples). For example if we start with 5 candidates, we only need 2 iterations: 5 candidates for the first iteration, then 5 // 2 = 2 candidates at the second iteration, after which we know which candidate performs the best (so we don’t need a third one). We would only be using at most 20 samples which is a waste since we have 1000 samples at our disposal. On the other hand, if we start with a high number of candidates, we might end up with a lot of candidates at the last iteration, which may not always be ideal: it means that many candidates will run with the full resources, basically reducing the procedure to standard search.
 The number of candidates is specified directly in HalvingRandomSearchCV, and is determined from the param_grid parameter of HalvingGridSearchCV.
The number of candidates in HalvingRandomSearchCV is set by default such that the last iteration uses as much of the available resources as possible or is specified directly. _For HalvingGridSearchCV, the number of candidates is determined by the param_grid parameter_. Changing the value of `min_resources` will impact the number of possible iterations, and as a result will also have an effect on the ideal number of candidates.

Another consideration when choosing min_resources is whether or not it is easy to discriminate between good and bad candidates with a small amount of resources. For example, if you need a lot of samples to distinguish between good and bad parameters, a high min_resources is recommended. On the other hand if the distinction is clear even with a small amount of samples, then a small min_resources may be preferable since it would speed up the computation.

Notice in the example above that the last iteration does not use the maximum amount of resources available: 1000 samples are available, yet only 640 are used, at most. By default, both HalvingRandomSearchCV and HalvingGridSearchCV try to use as many resources as possible in the last iteration, with the constraint that this amount of resources must be a multiple of both min_resources and factor. HalvingRandomSearchCV achieves this by sampling the right amount of candidates, while HalvingGridSearchCV achieves this by properly setting min_resources. 

##### Aggressive elimination of candidates
Ideally, we want the last iteration to evaluate `factor` candidates. We then just have to pick the best one. When the number of available resources is small with respect to the number of candidates, the last iteration may have to evaluate more than `factor` candidates. Using the `aggressive_elimination` parameter, you can force the search process to end up with less than `factor` candidates at the last iteration. To do this, the process will eliminate as many candidates as necessary using `min_resources` resources.

##### Analyzing results with the cv_results_ attribute
The `cv_results_` attribute contains useful information for analyzing the results of a search. It can be converted to a pandas dataframe with `df = pd.DataFrame(est.cv_results_)`

**Notes**
- (RandomizedSearchCV) For continuous parameters, it is important to specify a continuous distribution to take full advantage of the randomization. This way, increasing n_iter will always lead to a finer search. This can be done using scipy.stats module which contains many useful distributions for sampling parameters such as expon, gamma, uniform, loguniform or randint. A continuous log-uniform random variable is the continuous version of a log-spaced parameter. For example, loguniform(1, 100) can be used instead of [1, 10, 100].
- When using ensemble methods base upon bagging, i.e. generating new training sets using sampling with replacement, part of the training set remains unused. For each classifier in the ensemble, a different part of the training set is left out. This left out portion can be used to estimate the generalization error without having to rely on a separate validation set. This estimate comes “for free” as no additional data is needed and can be used for model selection. [Check here for models that support this](https://scikit-learn.org/stable/modules/grid_search.html#out-of-bag-estimates)
-After all the fine tuning, it would be good to ensure that your model not only works well on average, but also on all categories. Eg in our test project, the model should work on all categories of districts, whether they’re rural or urban, rich or poor, North or South, minority or not, etc. This requires a bit of work creating subsets of your validation set for each category, but it’s important: if your model performs poorly on a whole category of districts, then it should probably not be deployed until the issue is solved, or at least it should not be used to make predictions for that category, as it may do more harm than good.

### Model Selection
When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a “validation set”. This holdout validation process involves simply holding out part of the training set to evaluate several candidate models and select the best one. The new held-out set is called the validation set (or sometimes the development set, or dev set). More specifically, you train multiple models with various hyperparameters on the reduced training set (i.e., the full training set minus the validation set), and you select the model that performs best on the validation set. After this holdout validation process, **you train the best model on the full training set (including the validation set)**, and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error. This usually works quite well. However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, if the validation set is too small, then model evaluations will be imprecise: you may end up selecting a suboptimal model by mistake. Conversely, if the validation set is too large, then the remaining training set will be much smaller than the full training set. This is bad because since the final model will be trained on the full training set, **it is not ideal to compare candidate models trained on a much smaller training set.** It would be like selecting the fastest sprinter to participate in a marathon. One way to solve this problem is to perform repeated cross-validation: In this approach(called k-fold CV), the validation set is no longer needed as the training set is split into k smaller sets. Each model is evaluated once per set after it is trained on the rest of the data. By averaging out all the evaluations of a model, you get a much more accurate measure of its performance.  However, the training time is multiplied by the number of k sets.

The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset.It primarily evaluates a model's performance by calculating a single score across multiple cross validation folds.
The cross_validate function works in the same way as cross_val_score but differs in two ways:
1. It allows specifying multiple metrics for evaluation.
2. It returns a dict containing fit-times, score-times (and optionally training scores, fitted estimators, train-test split indices) in addition to the test score. Hence it is more versatile as it provides a wider range of information about the model's performance.

The function cross_val_predict has a similar interface to cross_val_score, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).

_* Warning Note on inappropriate usage of cross_val_predict *_
*The result of cross_val_predict may be different from those obtained using cross_val_score as the elements are grouped in different ways. The function cross_val_score takes an average over cross-validation folds, whereas cross_val_predict simply returns the labels (or probabilities) from several distinct models undistinguished. Thus, cross_val_predict is not an appropriate measure of generalization error.*

The function cross_val_predict is appropriate for:
- Model Assessment: Visualization of predictions obtained from different models and assessing how well your model generalizes to unseen data across different folds of the dataset. By obtaining predictions for each fold, you can evaluate whether your model consistently makes accurate predictions or if its performance varies between different subsets of your data
- Model blending: When predictions of one supervised estimator are used as inputs to train another estimator in ensemble methods, cross_val_predict helps obtain these predictions in a cross-validated manner to avoid overfitting.

### Validation and Learning Curves
- it is sometimes helpful to plot the influence of a single hyperparameter on the training score and the validation score to find out whether the estimator is overfitting or underfitting for some hyperparameter values. The sklearn.model_selection.validation_curve function  can help in this case. If you intend to plot the validation curves only, the sklearn.model_selection.ValidationCurveDisplay class is more direct than using matplotlib manually on the results of a call to validation_curve. You can use its from_estimator method similarly to validation_curve to generate and plot the validation curve.

- A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is quite low with increasing size of the training set. Then, we will probably not benefit much from more training data. In contrast, for small amounts of data, if the training score is much greater than the validation score. Adding more training samples will most likely increase generalization. You can use learning_curve and LearningCurveDisplay in pretty much the same way as that of validation curves.
